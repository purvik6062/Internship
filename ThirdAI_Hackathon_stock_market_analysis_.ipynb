{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zui1_i4MjVaD"
      },
      "source": [
        "## ThirdAI's NeuralDB\n",
        "\n",
        "NeuralDB, as the name suggests, is a combination of a neural network and a database. It provides a high-level API for users to insert different types of files into it and search through the file contents with natural language queries. The neural network part of it enables semantic search while the database part of it stores the paragraphs of the files that are inserted into it.\n",
        "\n",
        "First, let's install the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zAoAP1wjVaF"
      },
      "outputs": [],
      "source": [
        "!pip3 install thirdai --upgrade\n",
        "!pip3 install thirdai[neural_db]\n",
        "!pip3 install langchain --upgrade\n",
        "!pip3 install openai --upgrade\n",
        "!pip3 install paper-qa --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88FjJtvCjVaF"
      },
      "outputs": [],
      "source": [
        "from thirdai import licensing, neural_db as ndb\n",
        "licensing.deactivate()\n",
        "licensing.activate(\"1FB7DD-CAC3EC-832A67-84208D-C4E39E-V3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2oh7JXJjVaG"
      },
      "source": [
        "Now, let's import the relevant module and define a neural db class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez7C5yAKjVaG"
      },
      "outputs": [],
      "source": [
        "db = ndb.NeuralDB(user_id=\"my_user\") # you can use any username, in the future, this username will let you push models to the model hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtC3eAIkjVaG"
      },
      "source": [
        "### You even load from a base DB from our Bazaar (optional but recommended)\n",
        "\n",
        "We have a model bazaar that provides users with domain specific NeuralDBs that can jumpstart searching on their private documents. The Bazaar has two main types of DBs\n",
        "\n",
        "1. Base DBs: These come with models that have either general QnA capabilities or domain specific capabilities like search on Medical Documents, Financial documents or Contracts. These come with an empty data index into which users can insert their files.\n",
        "\n",
        "2. Pre-Indexed DBs: These are ready-to-search DBs that come with pre-trained models and their corresponding datasets. These are meant to  search through large public datasets like PubMed or Amazon 3MM Products or Stackoverflow issues etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjSBCgxjjVaG"
      },
      "outputs": [],
      "source": [
        "# Set up a cache directory\n",
        "import os\n",
        "if not os.path.isdir(\"bazaar_cache\"):\n",
        "    os.mkdir(\"bazaar_cache\")\n",
        "\n",
        "from pathlib import Path\n",
        "from thirdai.neural_db import Bazaar\n",
        "bazaar = Bazaar(cache_dir=Path(\"bazaar_cache\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMBhmBfDjVaH"
      },
      "source": [
        "Call fetch to refresh list of available DBs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYVb_JmFjVaH"
      },
      "outputs": [],
      "source": [
        "bazaar.fetch() # Optional arg filter=\"model name\" to filter by model name.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wLYW2kfjVaH"
      },
      "source": [
        "Below is the list of all DBs in the Bazaar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBK5CQGwjVaH"
      },
      "outputs": [],
      "source": [
        "print(bazaar.list_model_names())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmjwqeC0jVaI"
      },
      "source": [
        "Finally load the DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NLtXNyEjVaI"
      },
      "outputs": [],
      "source": [
        "db = bazaar.get_model(\"General QnA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK2qt9SnjVaI"
      },
      "source": [
        "### Insert your files\n",
        "\n",
        "Let's insert things into it!\n",
        "\n",
        "Currently, we natively support adding CSV, PDF and DOCX files. We also have a support to automatically scrape and parse URLs. All other file formats have to be converted into CSV files where each row represents a paragraph/text-chunk of the document."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "1cQE0bEol8bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcLYFGoHjVaI"
      },
      "outputs": [],
      "source": [
        "insertable_docs = []\n",
        "csv_files = ['Stocks_Dataset.csv']\n",
        "\n",
        "for file in csv_files:\n",
        "    csv_doc = ndb.CSV(\n",
        "        path=file,\n",
        "        id_column=\"DOC_ID\",\n",
        "        strong_columns=[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"Name\"],\n",
        "        weak_columns=[\"high\", \"low\"],\n",
        "        reference_columns=[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"Name\"])\n",
        "    #\n",
        "    insertable_docs.append(csv_doc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUCyzI8ajVaI"
      },
      "source": [
        "#### Example 2: PDF files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJFilNwhjVaI"
      },
      "outputs": [],
      "source": [
        "insertable_docs = []\n",
        "pdf_files = ['analysis.pdf']\n",
        "\n",
        "for file in pdf_files:\n",
        "    pdf_doc = ndb.PDF(file)\n",
        "    insertable_docs.append(pdf_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWkemY4WjVaJ"
      },
      "source": [
        "### Insert into NeuralDB\n",
        "\n",
        "If you wish to insert without unsupervised training, you can set 'train=False' in the insert() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y51pFUB3jVaJ"
      },
      "outputs": [],
      "source": [
        "source_ids = db.insert(insertable_docs, train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3I2nkfhjVaJ"
      },
      "source": [
        "The above command is intended to be used with a base DB which already has reasonable knowledge of the domain. In general, we always recommend using 'train=True' as shown below.\n",
        "\n",
        "#### Insert and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9e0Eu21jVaJ"
      },
      "outputs": [],
      "source": [
        "source_ids = db.insert(insertable_docs, train=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tcN00b1jVaJ"
      },
      "source": [
        "If you call the insert() method multiple times, the documents will automatically be de-duplicated. If insert=True, then the training will be done multiple times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12S7EGyfjVaJ"
      },
      "source": [
        "### Search\n",
        "\n",
        "Now let's start searching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFrzHytJjVaJ",
        "outputId": "953c8a9f-4794-45d7-968c-8090e55bd5e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From the essence of OHLC data it represents the prices of certain financial product. In- vestors continue to make purchases and sells according to accurate predictions of OHLC data and thus earn profits is the fundamental incentive mechanism to maintain its effective operation (Liu et al. 2017).\n",
            "************\n",
            "Moreover it not only preserves a positive relative relationship between the original data as logarithm transformation is a monotonous increasing function but also compresses the scale of the data which reduces the absolute values of the original data and makes the data more stable to some extent.\n",
            "************\n"
          ]
        }
      ],
      "source": [
        "search_results = db.search(\n",
        "    query=\"what was in the dataset?\",\n",
        "    top_k=2,\n",
        "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
        "\n",
        "for result in search_results:\n",
        "    print(result.text)\n",
        "    # print(result.context(radius=1))\n",
        "    # print(result.source)\n",
        "    # print(result.metadata)\n",
        "    print('************')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxWSuMFejVaJ"
      },
      "source": [
        "We can see that the search pulled up the right passage that contains the termination period \"(i) five (5) years or (ii) when the confidential information no longer qualifies as a trade secret\" ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8a5ltFfjVaK",
        "outputId": "8c382f20-adaf-41e8-cf60-e528398a0df5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If limit-up(limit-down) happens we firstly multiply x(c)(x(o)) and x(h) by 1.1 to make a relatively large interval. And then conduct measurements given in circumstances (2) and (3). In summary the general forecasting framework for OHLC data with T periods is described in Algorithm 1.\n",
            "************\n",
            "Forecasting and trading the high-low range of stocks and etfs with neural networks. In International Conference on Engineering Applications of Neural Networks pages 423-432. Springer 2012. Hans-J\"org von Mettenheim and Michael H Breitner. Forecasting daily highs and lows of liquid assets with neural networks.\n",
            "************\n"
          ]
        }
      ],
      "source": [
        "search_results = db.search(\n",
        "    query=\"General forecasting framework for OHLC data\",\n",
        "    top_k=2,\n",
        "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
        "\n",
        "for result in search_results:\n",
        "    print(result.text)\n",
        "    # print(result.context(radius=1))\n",
        "    # print(result.source)\n",
        "    # print(result.metadata)\n",
        "    print('************')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpZBT47ojVaK"
      },
      "source": [
        "We can see that the search pulled up the right passage again that has \"made by and between\".\n",
        "\n",
        "Now let's ask a tricky question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSezw5oFjVaK",
        "outputId": "5eaaef34-9fac-4a0c-84d8-71e160c0ea32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A trade-off must be evaluated to choose p the common used criterions in practice are AIC BIC and HQ (Hannan-Quinn). In this paper we prefer AIC because of its conciseness which is formulated as AIC(p) = ln 4 i=1 T j=1 ^u2 ij T + 2pK2 T (15) 14 where T stands for the total period number of OHLC series p is VAR lag order K is the VAR dimension and ^uij = ^Y (i) j - Y (i) j (1 <= i <= 4 1 <= j <= T) represents for the residuals of the VAR model.\n",
            "************\n",
            "Finally the simulated OHLC data {Xt}T t=1 are generated by applying the inverse transformation formula in Eq. (9). In order to evaluate the performance of the proposed method with different variance com- ponent levels we consider the following scenarios: Scenario 1: p = 1 T = 220 Y1 = [4 0.7 -0.85 0]T and A1 =             0.55 0.12 0.12 0.12 0.12 0.55 0.12 0.12 0.12 0.12 0.55 0.12 0.12 0.12 0.12 0.55             and Sw is a 4 x 4 diagonal matrix with diagonal element being 0.052 i.e. Sw = diag{0.052 0.052 0.052 0.052}.\n",
            "************\n"
          ]
        }
      ],
      "source": [
        "search_results = db.search(\n",
        "    query=\"AIC is formulated as\",\n",
        "    top_k=2,\n",
        "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
        "\n",
        "for result in search_results:\n",
        "    print(result.text)\n",
        "    # print(result.context(radius=1))\n",
        "    # print(result.source)\n",
        "    # print(result.metadata)\n",
        "    print('************')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZpQcs4CjVaZ"
      },
      "source": [
        "### Get Answers from OpenAI using Langchain\n",
        "\n",
        "In this section, we will show how to use LangChain and query OpenAI's QnA module to generate an answer from the references that you retrieve from the above DB. You'll have to specify your own OpenAI key for this module to work. You can replace this segment with any other generative model of your choice. You can choose to use an source model like MPT or Dolly for answer generation with the same prompt that you use with OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xM3LiMPjVaZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cgAxYF5jVaZ"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from paperqa.prompts import qa_prompt\n",
        "from paperqa.chains import make_chain\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "qa_chain = make_chain(prompt=qa_prompt, llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cg1UqS1jVaZ"
      },
      "outputs": [],
      "source": [
        "def get_references(query):\n",
        "    search_results = db.search(query,top_k=3)\n",
        "    references = []\n",
        "    for result in search_results:\n",
        "        references.append(result.text)\n",
        "    return references\n",
        "\n",
        "def get_answer(query, references):\n",
        "    return qa_chain.run(question=query, context='\\n\\n'.join(references[:3]), answer_length=\"abt 50 words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sc0nTgawjVaa",
        "outputId": "c7607db7-312c-4ab9-bc52-d76ff48f90fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A trade-off must be evaluated to choose p the common used criterions in practice are AIC BIC and HQ (Hannan-Quinn). In this paper we prefer AIC because of its conciseness which is formulated as AIC(p) = ln 4 i=1 T j=1 ^u2 ij T + 2pK2 T (15) 14 where T stands for the total period number of OHLC series p is VAR lag order K is the VAR dimension and ^uij = ^Y (i) j - Y (i) j (1 <= i <= 4 1 <= j <= T) represents for the residuals of the VAR model.', 'Finally the simulated OHLC data {Xt}T t=1 are generated by applying the inverse transformation formula in Eq. (9). In order to evaluate the performance of the proposed method with different variance com- ponent levels we consider the following scenarios: Scenario 1: p = 1 T = 220 Y1 = [4 0.7 -0.85 0]T and A1 =             0.55 0.12 0.12 0.12 0.12 0.55 0.12 0.12 0.12 0.12 0.55 0.12 0.12 0.12 0.12 0.55             and Sw is a 4 x 4 diagonal matrix with diagonal element being 0.052 i.e. Sw = diag{0.052 0.052 0.052 0.052}.', 'Obviously without properly processing the raw data it is very likely that the predicted open/close price is beyond boundaries. To remedy this situation inquired by the idea of convex combination here we introduce two proxy data l(o) t and l(c) t which are formulated as l(o) t = x(o) t - x(l) t x(h) t - x(l) t and l(c) t = x(c) t - x(l) t x(h) t - x(l) t .']\n"
          ]
        }
      ],
      "source": [
        "query = \"AIC is formulated as\"\n",
        "\n",
        "references = get_references(query)\n",
        "print(references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lF9V-B9kjVaa",
        "outputId": "387fa23b-40bd-4c90-b482-a929e5679961",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AIC is formulated as AIC(p) = ln 4 i=1 T j=1 ^u2 ij T + 2pK2 T (15), where p is the VAR lag order, T is the total period number of OHLC series, K is the VAR dimension, and ^uij represents the residuals of the VAR model (Example2012).\n"
          ]
        }
      ],
      "source": [
        "answer = get_answer(query, references)\n",
        "\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRXeHXmYjVaa"
      },
      "source": [
        "### Load and Save\n",
        "As usual, saving and loading the DB are one-liners."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nnxo86BjVaa"
      },
      "outputs": [],
      "source": [
        "# save your db\n",
        "db.save(\"data.db\")\n",
        "\n",
        "# Loading is just like we showed above, with an optional progress handler\n",
        "db.from_checkpoint(\"data.db\", on_progress=lambda fraction: print(f\"{fraction}% done with loading.\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}